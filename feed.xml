<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://judepark96.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://judepark96.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-08-28T08:41:34-05:00</updated><id>https://judepark96.github.io/blog/feed.xml</id><title type="html">Eunhwan Park X NLP</title><subtitle>Eunhwan Park X NLP</subtitle><entry><title type="html">TorchText Field 의 batch_first 는 항상 True 로 설정하자</title><link href="https://judepark96.github.io/blog/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first.html" rel="alternate" type="text/html" title="TorchText Field 의 batch_first 는 항상 True 로 설정하자" /><published>2020-08-25T00:00:00-05:00</published><updated>2020-08-25T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first.html">&lt;h2 id=&quot;1-소개&quot;&gt;1. 소개&lt;/h2&gt;

&lt;p&gt;우선 평소에 따로 vocabulary, tokenizer 등의 데이터 전처리 과정에서 필요한 요소들을 직접 코딩해왔었다. 하지만, 이번에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TorchText&lt;/code&gt; 를 한 번 사용해보고자 했다. 그 과정에서 나의 어이없는 실수를 소개한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;한줄 요약: torchtext.data.Field() 를 사용할 때 항상 batch_first=True 로 설정하자.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-어떤-문제가-발생했어&quot;&gt;2. 어떤 문제가 발생했어?&lt;/h2&gt;

&lt;p&gt;우선 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; 학습 데이터를 구축해놓고 이를 TorchText 를 통하여 불렀다. 학습 데이터의 내부 구조는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'text':'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'text':'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 어떻게 불러오는지 한 번 보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TabularDataset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;sos&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eos_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;eos&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TabularDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data.json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                                 &lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                                 &lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;우선 SRC와 TRG에 해당하는 Field 를 설정한다. TorchText 에서는 Field 내부의 속성들을 지정함으로서 데이터 전처리 의 번거로움을 줄여준다고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;위의 코드에선 batch_first=True 로 하였다. False 일 때와 차이점을 아래에서 보여줄 예정이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 우리가 일상적으로 사용하는 텐서의 형태로 보기 위해서는 아래와 같이 코드를 작성한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;우리가 만들었어야하는 vocabulary 를 위의 2줄로 만들 수가 있다. 옴마가쉬… 그 외에도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_freq&lt;/code&gt; 와 같은 속성으로 vocabulary 생성 과정에 조건을 걸 수 있다.&lt;/p&gt;

&lt;p&gt;이제 iterator 로 돌려보고 결과를 보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_iterator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BucketIterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text -&amp;gt; to &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; , a large number of ... have been stated and the most appropriate algorithm &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;each application has been determined &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

label -&amp;gt; &amp;lt;sos&amp;gt; autonomous exploration __&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;__ mapping of unknown environments &amp;lt;eos&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;대충 의도한대로 제대로 나왔다. 그렇다면 만약 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_first=False&lt;/code&gt; 로 설정한다면 어떻게 될까?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;텍스트 길이가 길기 때문에 일부 텍스트와 패딩 토큰을 보기 좋게 하기 위하여 지웠다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt;
&amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_first=False&lt;/code&gt; 를 하였을 때는 위처럼 나오게 된다. 옴마가쉬…&lt;/p&gt;

&lt;h2 id=&quot;3-여기서-나의-실수는&quot;&gt;3. 여기서 나의 실수는?&lt;/h2&gt;

&lt;p&gt;많은 예제들, 그리고 나의 경험에서 나는 항상 데이터 전처리 과정에서 텐서의 형상을 $\text{batch_size} \ntimes \text{sequence_length}$ 의 형태를 띄도록 하였다. 하지만 TorchText 에서는 $\text{sequence_length} \ntimes \text{batch_size}$ 의 형태 또한 지원했던 것이다. 그렇기 때문에 Index -&amp;gt; Text 로 변환하였을 때 제대로 표현이 안되었던 것이다. 홀리쉿…&lt;/p&gt;

&lt;h2 id=&quot;4-결론&quot;&gt;4. 결론&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;batch_first&lt;/strong&gt; 속성을 확인하자.&lt;/li&gt;
  &lt;li&gt;TorchText 는 좋은 거다. 나름대로.&lt;/li&gt;
  &lt;li&gt;토큰화 과정은 &lt;strong&gt;따로 전처리하는 것&lt;/strong&gt;이 나은 것 같다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. 소개</summary></entry><entry><title type="html">Adding Special Tokens To HuggingFace BertTokenizer</title><link href="https://judepark96.github.io/blog/nlp/2020/08/18/Adding-Special-Tokens-To-HuggingFace-BertTokenizer.html" rel="alternate" type="text/html" title="Adding Special Tokens To HuggingFace BertTokenizer" /><published>2020-08-18T00:00:00-05:00</published><updated>2020-08-18T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp/2020/08/18/Adding%20Special%20Tokens%20To%20HuggingFace%20BertTokenizer</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp/2020/08/18/Adding-Special-Tokens-To-HuggingFace-BertTokenizer.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Sometimes, except pre-defined special tokens, such as &lt;strong&gt;[SEP], [CLS]&lt;/strong&gt;, we need to add special tokens to BertTokenizer. &lt;strong&gt;In this post, I Introduce the way to add special tokens to BertTokenizer.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-method&quot;&gt;2. Method&lt;/h2&gt;

&lt;p&gt;First, we need to define speical token what we will add. In this post, adding special token will be &lt;strong&gt;‘[eou]’&lt;/strong&gt; which means &lt;strong&gt;End of Utterances&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eou_token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'[eou]'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, add eou_token to BertTokenizer.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;special_tokens_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'additional_special_tokens'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode_sep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;orig_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_added_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special_tokens_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orig_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_added_tokens&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We got the tokenizer we added the special tokens, and total number of vocab. After this stage, we need to change the embedding size of BertModel.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize_token_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_num_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Done!&lt;/p&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Graph Convolutional Networks for Text Classification</title><link href="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph-Convolutional-Networks-for-Text-Classification.html" rel="alternate" type="text/html" title="Graph Convolutional Networks for Text Classification" /><published>2020-08-14T00:00:00-05:00</published><updated>2020-08-14T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph%20Convolutional%20Networks%20for%20Text%20Classification</id><content type="html" xml:base="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph-Convolutional-Networks-for-Text-Classification.html">&lt;h1 id=&quot;1-초록은-뭐라고-말하고-있어-&quot;&gt;1. 초록은 뭐라고 말하고 있어 ?&lt;/h1&gt;

&lt;p&gt;기존의 텍스트 분류 과업을 하기 위한 몇 몇 연구에서는 convolutional neural networks 를 사용하였다. 본 논문에서는 convolutional neural networks 가 아닌 graph convolutional networks 를 텍스트 분류 과업에 적용하는 것을 제안한다.&lt;/p&gt;

&lt;h1 id=&quot;2-주요-기여점은-뭐야-&quot;&gt;2. 주요 기여점은 뭐야 ?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;We propose a novel graph neural network method for text classification. To the best of our knowledge, this is the first study to model a whole corpus as a heterogeneous graph and learn word and document embeddings with graph neural networks jointly.&lt;/li&gt;
  &lt;li&gt;Results on several benchmark datasets demonstrate that our method outperforms state-of-the-art text classification methods, without using pre-trained word embeddings or external knowledge. Our method also learn predictive word and document embeddings automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3-이전의-접근과는-뭐가-다른-것-같아-&quot;&gt;3. 이전의 접근과는 뭐가 다른 것 같아 ?&lt;/h1&gt;

&lt;p&gt;기존의 텍스트 분류 과업을 위한 주요 딥러닝 모델로는 CNN, LSTM 등이 있다. 이러한 모델들은 local consecutive word sequences 의 semantic, synthetic information 을 잘 caputre 할 수 있지만 global word co-occurence 를 무시할 수 있는 가능성이 있다.&lt;/p&gt;

&lt;p&gt;본 논문에서는 이전의 접근과는 다르게 graph representation 을 학습하여 classification task 를 수행한다는 점이다. 이를 수행하기 위하여 corpus -&amp;gt; graph 의 방법으로 PMI, TF-IDF 를 소개하였고, PMI 의 경우가 성능이 더 좋음을 알려준다. 또한 two-layer GCN 이 one-layer GCN 보다 성능이 좋았지만 더 많은 layer 는 성능에 큰 의미가 없음을 나타내었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;기존의 CNN, LSTM 을 GNN 계열의 신경망으로 대체한다는 것 자체가 geometric 의 관점으로 보았을 때 큰 차이가 있다.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-어떤-걸-제안할-수-있을까-&quot;&gt;4. 어떤 걸 제안할 수 있을까 ?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;encoding method (corpus -&amp;gt; graph)&lt;/li&gt;
  &lt;li&gt;residual connection&lt;/li&gt;
  &lt;li&gt;한국어 데이터 집합에 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;5-reference&quot;&gt;5. Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;&gt;Graph Convolutional Networks for Text Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. 초록은 뭐라고 말하고 있어 ?</summary></entry><entry><title type="html">What is Page Rank?</title><link href="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html" rel="alternate" type="text/html" title="What is Page Rank?" /><published>2020-08-12T00:00:00-05:00</published><updated>2020-08-12T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR</id><content type="html" xml:base="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html">&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;PageRank (PR) is an algorithm used to calculate the weight for web pages.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-pr-equation&quot;&gt;2. PR Equation&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(V_i) = (1-d) + d * \sum_{j\in In(v_i)}\frac{1}{|Out(v_j)|}S(V_j)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Denotes
    &lt;ul&gt;
      &lt;li&gt;$S(V_i)$ ← the weight of webpage $i$&lt;/li&gt;
      &lt;li&gt;$d$ ← damping factor, in case of no outgoing links&lt;/li&gt;
      &lt;li&gt;$In(v_i)$ ← inbound links of $i$, which is a set.&lt;/li&gt;
      &lt;li&gt;$Out(v_j)$ ← outgoing links of $j$, which is a set.&lt;/li&gt;
      &lt;li&gt;$\vert Out(v_j) \vert$ ← the number of outbound links&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;21-example&quot;&gt;2.1. Example&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1030/1*DkZjlRNEjPSc8RNL7yWggA.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight of node $e$ is below as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;In(v_e) = [a, b], j\in [a, b] \\
S(V_e) = (1-d) + d*(S(V_a) + \frac{1}{2}S(V_b))&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In the initialization, the weight of each node is $1$.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-pagerank-for-keyword-extraction-by-python&quot;&gt;3. PageRank for Keyword Extraction by Python&lt;/h1&gt;

&lt;p&gt;You can see the code at &lt;a href=&quot;https://github.com/JudePark96/textrank-nlp&quot;&gt;textrank-nlp&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;Understand TextRank for Keyword Extraction by Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Machine Translation, Sequence-to-Sequence and Attention</title><link href="https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html" rel="alternate" type="text/html" title="Machine Translation, Sequence-to-Sequence and Attention" /><published>2020-08-01T00:00:00-05:00</published><updated>2020-08-01T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine%20Translation,%20Sequence-to-Sequence%20and%20Attention</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Seq2Seq 공부를 하는데 블로그들은 알아보기 &lt;strong&gt;(내 기준)&lt;/strong&gt; 힘들어서 CS224n 의 강의 슬라이드를 보면서 정리하였다. 근데 내 정리도 보니까 나 혼자밖에 못 알아볼 듯.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-conclusion&quot;&gt;2. Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Neural Machine Translation 은 Sequence-to-Sequence 형태의 Neural Network 로 접근함.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Dialogue, Summarization 등의 다양한 Task 에 사용될 수 있음.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Conditional Language Model 이라고 볼 수 있음.&lt;/li&gt;
  &lt;li&gt;Decoding 에서 항상 optimal solution 이라고 보장할 수는 없지만 beam search 를 이용하는 것이 효율적임.&lt;/li&gt;
  &lt;li&gt;Evaluation Metric 으로서 BLEU 를 사용하지만 불완전함.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://judepark96.github.io/blog/2020/03/22/BLEU-Score-Reasonable.html&quot;&gt;BLEU (Bilingual Evaluation Understudy) Score&lt;/a&gt; 를 참고.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Information Bottleneck 을 Attention Mecahnism 을 이용하여 해소함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture08-nmt.pdf&quot;&gt;CS224n Lecture 8 Slide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Ambiguity In Sentence Structure</title><link href="https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html" rel="alternate" type="text/html" title="Ambiguity In Sentence Structure" /><published>2020-07-28T00:00:00-05:00</published><updated>2020-07-28T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity%20In%20Sentence%20Structure</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html">&lt;h3 id=&quot;1-attachment-ambiguity&quot;&gt;1. Attachment ambiguity&lt;/h3&gt;

&lt;p&gt;A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.&lt;/p&gt;

&lt;h3 id=&quot;2-coordination-ambiguity&quot;&gt;2. Coordination ambiguity&lt;/h3&gt;

&lt;p&gt;In coordination ambiguity different sets of phrases can be conjoined by a con- junction like &lt;em&gt;and&lt;/em&gt;. For example, the phrase &lt;strong&gt;&lt;em&gt;old men and women&lt;/em&gt;&lt;/strong&gt; can be bracketed as &lt;em&gt;[old [men and women]]&lt;/em&gt;, referring to &lt;strong&gt;&lt;em&gt;old men&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;old women&lt;/em&gt;&lt;/strong&gt;, or as &lt;strong&gt;&lt;em&gt;[old men] and [women]&lt;/em&gt;,&lt;/strong&gt; in which case it is only the men who are old.&lt;/p&gt;

&lt;h3 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Attachment ambiguity</summary></entry><entry><title type="html">하반기 연구 주제를 위한 공부 목록</title><link href="https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html" rel="alternate" type="text/html" title="하반기 연구 주제를 위한 공부 목록" /><published>2020-07-06T00:00:00-05:00</published><updated>2020-07-06T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D</id><content type="html" xml:base="https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html">&lt;ul&gt;
  &lt;li&gt;연구 주제
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Keyphrase 추출/생성&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Named Entity Recognition&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;공부 목록
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Stanford NLP&lt;/a&gt; chap 2-3, 8, 12-15&lt;/li&gt;
      &lt;li&gt;관련 논문 읽기&lt;/li&gt;
      &lt;li&gt;ICLR 2020 최근 논문 읽기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해야할 일
    &lt;ul&gt;
      &lt;li&gt;연구 주제 실험&lt;/li&gt;
      &lt;li&gt;다양한 연구 주제 생각&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">연구 주제 Keyphrase 추출/생성 Named Entity Recognition 공부 목록 Stanford NLP chap 2-3, 8, 12-15 관련 논문 읽기 ICLR 2020 최근 논문 읽기 해야할 일 연구 주제 실험 다양한 연구 주제 생각</summary></entry><entry><title type="html">Bias-Variance Trade off</title><link href="https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff.html" rel="alternate" type="text/html" title="Bias-Variance Trade off" /><published>2020-06-22T00:00:00-05:00</published><updated>2020-06-22T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff</id><content type="html" xml:base="https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff.html">&lt;p&gt;Supervised Learning 을 할 때, 설계한 모델의 Predicted Value 와 실제 Label Value 간의 차이를 Error 라고 한다.&lt;/p&gt;

&lt;p&gt;이 Error 는 Variance, Bias, Noise 로 이루어져 있는데 $Error(x) = Var(x) + Bias(x) + Noise(x)$ 이다.&lt;/p&gt;

&lt;p&gt;Error 가 어떻게 위의 언급한 것과 같이 되는지 알아본다.&lt;/p&gt;

&lt;h1 id=&quot;decomposition&quot;&gt;Decomposition&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;$y = f(x) + \epsilon$
    &lt;ul&gt;
      &lt;li&gt;$y$ 는 $\text{noise } \epsilon$ 을 포함한 함수이다. 이 때, noise 는 평균이 0, 표준편차는 $\sigma^2$ 인 가우시안 분포를 따른다고 가정한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$g(x) = wx + b$
    &lt;ul&gt;
      &lt;li&gt;학습할 hypothesis function 이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$Error = E[(y-g(x))^2]$
    &lt;ul&gt;
      &lt;li&gt;hypothesis function 을 바탕으로 한 error function 이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$y’=f(x’)+\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(y'-g(x'))^2] \\
= E[g(x')^2 - 2g(x')y' + y'^2] \\
= Var(g(x')) + E[g(x')]^2 - 2E[g(x')]f(x') + Var(y')+f(x')^2 \\
= Var(g(x')) + (E[g(x')]-f(x'))^2 + Var(\epsilon) \\
= Var(g(x')) + (E[g(x')]-f(x'))^2 + \sigma^2&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;Low Bias-High Variance 인 상태일 수록 Overfitting 이고, High Bias-Low Variance 인 상태일 수록 Underfitting 이라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;우리는 적절한 Bias, Variance 를 갖는 모델을 설계해야함을 알 수 있다.&lt;/p&gt;</content><author><name></name></author><summary type="html">Supervised Learning 을 할 때, 설계한 모델의 Predicted Value 와 실제 Label Value 간의 차이를 Error 라고 한다.</summary></entry><entry><title type="html">Sigmoid Derivation</title><link href="https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid.html" rel="alternate" type="text/html" title="Sigmoid Derivation" /><published>2020-06-06T00:00:00-05:00</published><updated>2020-06-06T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid</id><content type="html" xml:base="https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid.html">&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \frac{1}{1+e^{-x}}&lt;/script&gt;

&lt;p&gt;우선 $u = 1+e^{-x}$ 로 한다면 아래와 같이 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \frac{1}{u}&lt;/script&gt;

&lt;p&gt;이제 이를 미분하는 과정은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dy}{du}(u^{-1})\frac{du}{dx}(1+e^{-x}) \\
=-u^{-2} (-e^{-x}) \\
=\frac{e^{-x}}{(1+e^{-x})^2} \\
=y(1-y)&lt;/script&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">여름 방학동안 뭐하지 ?</title><link href="https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99.html" rel="alternate" type="text/html" title="여름 방학동안 뭐하지 ?" /><published>2020-05-30T00:00:00-05:00</published><updated>2020-05-30T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99</id><content type="html" xml:base="https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99.html">&lt;p&gt;이번 글만큼은 약간 러프한 나의 생각을 나열해보자! 러프한 생각 나열이니 글씨 말투도 러프하게 하자!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;회사 인턴&lt;/li&gt;
  &lt;li&gt;AI Rush&lt;/li&gt;
  &lt;li&gt;인라이플 KorQuAD Challenge 마무리&lt;/li&gt;
  &lt;li&gt;EmotionGIF 2020&lt;/li&gt;
  &lt;li&gt;밑바닥부터 시작하는 딥러닝 1~2 복습&lt;/li&gt;
  &lt;li&gt;기초 수학 복습&lt;/li&gt;
  &lt;li&gt;운동 (aka. 건강한 삶)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;연구와 관련된 모든 것을 지탱할 수 있는 나의 비장의 무기.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/images/tfrc_tpu.png&quot; alt=&quot;trfc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하하하하!!! 꺄르륵!!! 내가 지금까지 이걸 아껴두고 있었지!!!&lt;/p&gt;

&lt;p&gt;열심히 해야징 쿄쿄쿄…&lt;/p&gt;</content><author><name></name></author><summary type="html">이번 글만큼은 약간 러프한 나의 생각을 나열해보자! 러프한 생각 나열이니 글씨 말투도 러프하게 하자!</summary></entry></feed>