<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://judepark96.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://judepark96.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-08-13T11:43:17-05:00</updated><id>https://judepark96.github.io/blog/feed.xml</id><title type="html">Eunhwan Park X NLP</title><subtitle>Eunhwan Park X NLP</subtitle><entry><title type="html">What is Page Rank?</title><link href="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html" rel="alternate" type="text/html" title="What is Page Rank?" /><published>2020-08-12T00:00:00-05:00</published><updated>2020-08-12T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR</id><content type="html" xml:base="https://judepark96.github.io/blog/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html">&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;PageRank (PR) is an algorithm used to calculate the weight for web pages.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-pr-equation&quot;&gt;2. PR Equation&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(V_i) = (1-d) + d * \sum_{j\in In(v_i)}\frac{1}{|Out(v_j)|}S(V_j)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Denotes
    &lt;ul&gt;
      &lt;li&gt;$S(V_i)$ ← the weight of webpage $i$&lt;/li&gt;
      &lt;li&gt;$d$ ← damping factor, in case of no outgoing links&lt;/li&gt;
      &lt;li&gt;$In(v_i)$ ← inbound links of $i$, which is a set.&lt;/li&gt;
      &lt;li&gt;$Out(v_j)$ ← outgoing links of $j$, which is a set.&lt;/li&gt;
      &lt;li&gt;$\vert Out(v_j) \vert$ ← the number of outbound links&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;21-example&quot;&gt;2.1. Example&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1030/1*DkZjlRNEjPSc8RNL7yWggA.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight of node $e$ is below as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;In(v_e) = [a, b], j\in [a, b] \\
S(V_e) = (1-d) + d*(S(V_a) + \frac{1}{2}S(V_b))&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In the initialization, the weight of each node is $1$.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-pagerank-for-keyword-extraction-by-python&quot;&gt;3. PageRank for Keyword Extraction by Python&lt;/h1&gt;

&lt;p&gt;You can see the code at &lt;a href=&quot;https://github.com/JudePark96/textrank-nlp&quot;&gt;textrank-nlp&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;Understand TextRank for Keyword Extraction by Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Machine Translation, Sequence-to-Sequence and Attention</title><link href="https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html" rel="alternate" type="text/html" title="Machine Translation, Sequence-to-Sequence and Attention" /><published>2020-08-01T00:00:00-05:00</published><updated>2020-08-01T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine%20Translation,%20Sequence-to-Sequence%20and%20Attention</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Seq2Seq 공부를 하는데 블로그들은 알아보기 &lt;strong&gt;(내 기준)&lt;/strong&gt; 힘들어서 CS224n 의 강의 슬라이드를 보면서 정리하였다. 근데 내 정리도 보니까 나 혼자밖에 못 알아볼 듯.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-conclusion&quot;&gt;2. Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Neural Machine Translation 은 Sequence-to-Sequence 형태의 Neural Network 로 접근함.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Dialogue, Summarization 등의 다양한 Task 에 사용될 수 있음.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Conditional Language Model 이라고 볼 수 있음.&lt;/li&gt;
  &lt;li&gt;Decoding 에서 항상 optimal solution 이라고 보장할 수는 없지만 beam search 를 이용하는 것이 효율적임.&lt;/li&gt;
  &lt;li&gt;Evaluation Metric 으로서 BLEU 를 사용하지만 불완전함.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://judepark96.github.io/blog/2020/03/22/BLEU-Score-Reasonable.html&quot;&gt;BLEU (Bilingual Evaluation Understudy) Score&lt;/a&gt; 를 참고.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Information Bottleneck 을 Attention Mecahnism 을 이용하여 해소함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture08-nmt.pdf&quot;&gt;CS224n Lecture 8 Slide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Ambiguity In Sentence Structure</title><link href="https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html" rel="alternate" type="text/html" title="Ambiguity In Sentence Structure" /><published>2020-07-28T00:00:00-05:00</published><updated>2020-07-28T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity%20In%20Sentence%20Structure</id><content type="html" xml:base="https://judepark96.github.io/blog/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html">&lt;h3 id=&quot;1-attachment-ambiguity&quot;&gt;1. Attachment ambiguity&lt;/h3&gt;

&lt;p&gt;A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.&lt;/p&gt;

&lt;h3 id=&quot;2-coordination-ambiguity&quot;&gt;2. Coordination ambiguity&lt;/h3&gt;

&lt;p&gt;In coordination ambiguity different sets of phrases can be conjoined by a con- junction like &lt;em&gt;and&lt;/em&gt;. For example, the phrase &lt;strong&gt;&lt;em&gt;old men and women&lt;/em&gt;&lt;/strong&gt; can be bracketed as &lt;em&gt;[old [men and women]]&lt;/em&gt;, referring to &lt;strong&gt;&lt;em&gt;old men&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;old women&lt;/em&gt;&lt;/strong&gt;, or as &lt;strong&gt;&lt;em&gt;[old men] and [women]&lt;/em&gt;,&lt;/strong&gt; in which case it is only the men who are old.&lt;/p&gt;

&lt;h3 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Attachment ambiguity</summary></entry><entry><title type="html">하반기 연구 주제를 위한 공부 목록</title><link href="https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html" rel="alternate" type="text/html" title="하반기 연구 주제를 위한 공부 목록" /><published>2020-07-06T00:00:00-05:00</published><updated>2020-07-06T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D</id><content type="html" xml:base="https://judepark96.github.io/blog/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html">&lt;ul&gt;
  &lt;li&gt;연구 주제
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Keyphrase 추출/생성&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Named Entity Recognition&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;공부 목록
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Stanford NLP&lt;/a&gt; chap 2-3, 8, 12-15&lt;/li&gt;
      &lt;li&gt;관련 논문 읽기&lt;/li&gt;
      &lt;li&gt;ICLR 2020 최근 논문 읽기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해야할 일
    &lt;ul&gt;
      &lt;li&gt;연구 주제 실험&lt;/li&gt;
      &lt;li&gt;다양한 연구 주제 생각&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">연구 주제 Keyphrase 추출/생성 Named Entity Recognition 공부 목록 Stanford NLP chap 2-3, 8, 12-15 관련 논문 읽기 ICLR 2020 최근 논문 읽기 해야할 일 연구 주제 실험 다양한 연구 주제 생각</summary></entry><entry><title type="html">Bias-Variance Trade off</title><link href="https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff.html" rel="alternate" type="text/html" title="Bias-Variance Trade off" /><published>2020-06-22T00:00:00-05:00</published><updated>2020-06-22T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff</id><content type="html" xml:base="https://judepark96.github.io/blog/machine-learning/2020/06/22/Bias-Variance-tradeoff.html">&lt;p&gt;Supervised Learning 을 할 때, 설계한 모델의 Predicted Value 와 실제 Label Value 간의 차이를 Error 라고 한다.&lt;/p&gt;

&lt;p&gt;이 Error 는 Variance, Bias, Noise 로 이루어져 있는데 $Error(x) = Var(x) + Bias(x) + Noise(x)$ 이다.&lt;/p&gt;

&lt;p&gt;Error 가 어떻게 위의 언급한 것과 같이 되는지 알아본다.&lt;/p&gt;

&lt;h1 id=&quot;decomposition&quot;&gt;Decomposition&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;$y = f(x) + \epsilon$
    &lt;ul&gt;
      &lt;li&gt;$y$ 는 $\text{noise } \epsilon$ 을 포함한 함수이다. 이 때, noise 는 평균이 0, 표준편차는 $\sigma^2$ 인 가우시안 분포를 따른다고 가정한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$g(x) = wx + b$
    &lt;ul&gt;
      &lt;li&gt;학습할 hypothesis function 이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$Error = E[(y-g(x))^2]$
    &lt;ul&gt;
      &lt;li&gt;hypothesis function 을 바탕으로 한 error function 이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$y’=f(x’)+\epsilon$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[(y'-g(x'))^2] \\
= E[g(x')^2 - 2g(x')y' + y'^2] \\
= Var(g(x')) + E[g(x')]^2 - 2E[g(x')]f(x') + Var(y')+f(x')^2 \\
= Var(g(x')) + (E[g(x')]-f(x'))^2 + Var(\epsilon) \\
= Var(g(x')) + (E[g(x')]-f(x'))^2 + \sigma^2&lt;/script&gt;

&lt;hr /&gt;

&lt;p&gt;Low Bias-High Variance 인 상태일 수록 Overfitting 이고, High Bias-Low Variance 인 상태일 수록 Underfitting 이라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;우리는 적절한 Bias, Variance 를 갖는 모델을 설계해야함을 알 수 있다.&lt;/p&gt;</content><author><name></name></author><summary type="html">Supervised Learning 을 할 때, 설계한 모델의 Predicted Value 와 실제 Label Value 간의 차이를 Error 라고 한다.</summary></entry><entry><title type="html">Sigmoid Derivation</title><link href="https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid.html" rel="alternate" type="text/html" title="Sigmoid Derivation" /><published>2020-06-06T00:00:00-05:00</published><updated>2020-06-06T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid</id><content type="html" xml:base="https://judepark96.github.io/blog/%EB%AF%B8%EB%B6%84/2020/06/06/sigmoid.html">&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \frac{1}{1+e^{-x}}&lt;/script&gt;

&lt;p&gt;우선 $u = 1+e^{-x}$ 로 한다면 아래와 같이 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \frac{1}{u}&lt;/script&gt;

&lt;p&gt;이제 이를 미분하는 과정은 아래와 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dy}{du}(u^{-1})\frac{du}{dx}(1+e^{-x}) \\
=-u^{-2} (-e^{-x}) \\
=\frac{e^{-x}}{(1+e^{-x})^2} \\
=y(1-y)&lt;/script&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">여름 방학동안 뭐하지 ?</title><link href="https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99.html" rel="alternate" type="text/html" title="여름 방학동안 뭐하지 ?" /><published>2020-05-30T00:00:00-05:00</published><updated>2020-05-30T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99</id><content type="html" xml:base="https://judepark96.github.io/blog/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99,/%EB%8C%80%ED%95%99%EC%83%9D/2020/05/30/%EC%97%AC%EB%A6%84%EB%B0%A9%ED%95%99.html">&lt;p&gt;이번 글만큼은 약간 러프한 나의 생각을 나열해보자! 러프한 생각 나열이니 글씨 말투도 러프하게 하자!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;회사 인턴&lt;/li&gt;
  &lt;li&gt;AI Rush&lt;/li&gt;
  &lt;li&gt;인라이플 KorQuAD Challenge 마무리&lt;/li&gt;
  &lt;li&gt;EmotionGIF 2020&lt;/li&gt;
  &lt;li&gt;밑바닥부터 시작하는 딥러닝 1~2 복습&lt;/li&gt;
  &lt;li&gt;기초 수학 복습&lt;/li&gt;
  &lt;li&gt;운동 (aka. 건강한 삶)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;연구와 관련된 모든 것을 지탱할 수 있는 나의 비장의 무기.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/images/tfrc_tpu.png&quot; alt=&quot;trfc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하하하하!!! 꺄르륵!!! 내가 지금까지 이걸 아껴두고 있었지!!!&lt;/p&gt;

&lt;p&gt;열심히 해야징 쿄쿄쿄…&lt;/p&gt;</content><author><name></name></author><summary type="html">이번 글만큼은 약간 러프한 나의 생각을 나열해보자! 러프한 생각 나열이니 글씨 말투도 러프하게 하자!</summary></entry><entry><title type="html">프로그래머스 - 모의고사 (Python)</title><link href="https://judepark96.github.io/blog/algorithm/2020/05/16/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%AA%A8%EC%9D%98%EA%B3%A0%EC%82%AC-(Python).html" rel="alternate" type="text/html" title="프로그래머스 - 모의고사 (Python)" /><published>2020-05-16T00:00:00-05:00</published><updated>2020-05-16T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/algorithm/2020/05/16/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4%20-%20%EB%AA%A8%EC%9D%98%EA%B3%A0%EC%82%AC%20(Python)</id><content type="html" xml:base="https://judepark96.github.io/blog/algorithm/2020/05/16/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%AA%A8%EC%9D%98%EA%B3%A0%EC%82%AC-(Python).html">&lt;h2 id=&quot;문제-설명&quot;&gt;문제 설명&lt;/h2&gt;

&lt;p&gt;수포자는 수학을 포기한 사람의 준말입니다. 수포자 삼인방은 모의고사에 수학 문제를 전부 찍으려 합니다. 수포자는 1번 문제부터 마지막 문제까지 다음과 같이 찍습니다.&lt;/p&gt;

&lt;p&gt;1번 수포자가 찍는 방식: 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, …
2번 수포자가 찍는 방식: 2, 1, 2, 3, 2, 4, 2, 5, 2, 1, 2, 3, 2, 4, 2, 5, …
3번 수포자가 찍는 방식: 3, 3, 1, 1, 2, 2, 4, 4, 5, 5, 3, 3, 1, 1, 2, 2, 4, 4, 5, 5, …&lt;/p&gt;

&lt;p&gt;1번 문제부터 마지막 문제까지의 정답이 순서대로 들은 배열 answers가 주어졌을 때, 가장 많은 문제를 맞힌 사람이 누구인지 배열에 담아 return 하도록 solution 함수를 작성해주세요.&lt;/p&gt;

&lt;h5 id=&quot;제한-조건&quot;&gt;제한 조건&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;시험은 최대 10,000 문제로 구성되어있습니다.&lt;/li&gt;
  &lt;li&gt;문제의 정답은 1, 2, 3, 4, 5중 하나입니다.&lt;/li&gt;
  &lt;li&gt;가장 높은 점수를 받은 사람이 여럿일 경우, return하는 값을 오름차순 정렬해주세요.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;입출력-예&quot;&gt;입출력 예&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;answers&lt;/th&gt;
      &lt;th&gt;return&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[1,2,3,4,5]&lt;/td&gt;
      &lt;td&gt;[1]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[1,3,2,4,2]&lt;/td&gt;
      &lt;td&gt;[1,2,3]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;입출력-예-설명&quot;&gt;입출력 예 설명&lt;/h5&gt;

&lt;p&gt;입출력 예 #1&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;수포자 1은 모든 문제를 맞혔습니다.&lt;/li&gt;
  &lt;li&gt;수포자 2는 모든 문제를 틀렸습니다.&lt;/li&gt;
  &lt;li&gt;수포자 3은 모든 문제를 틀렸습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 가장 문제를 많이 맞힌 사람은 수포자 1입니다.&lt;/p&gt;

&lt;p&gt;입출력 예 #2&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모든 사람이 2문제씩을 맞췄습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;문제-풀이&quot;&gt;문제 풀이&lt;/h2&gt;

&lt;p&gt;이 문제의 theme 은 &lt;strong&gt;완전탐색&lt;/strong&gt;이다. 주어진 패턴과 답안을 비교하며 맞은 수를 각 수포자마다 기록하고 형식에 맞게만 반환해주면 되는 문제이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;calc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;result_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;answers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;수포자가 찍는 방식을 패턴이라고 하자. 그렇다면 그 패턴은 최소한 &lt;strong&gt;패턴 *= (정답 수 / 패턴 길이) + 1&lt;/strong&gt; 만큼 될 것이다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p1~p3&lt;/code&gt; 가 그렇게 초기화된 것이다.&lt;/p&gt;

&lt;p&gt;그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;calc&lt;/code&gt; 이라는 lambda function 을 선언해준다. 이 lambda function 에서 맞은 것을 기록하며 만약 5개 중 2개가 맞았다면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[True, False, True, False, False]&lt;/code&gt; 와 같은 방식으로 기록될 것이다. (이것은 예시일 뿐이다.) 그렇다면 이 배열을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum&lt;/code&gt; 함수로 계산하면 2가 될 것이다.&lt;/p&gt;

&lt;p&gt;그리고 답안 형태에 맞게 변환해주고 반환해주면 정답이 풀린다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Python 의 Generator 를 이용하면 (cycle 과 같은) 공간복잡도 또한 해결할 수 있으나, 최대 10,000 문제라는 조건과 쉬운 문제의 특성 상 cycle 을 사용하지 않았다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42840&quot;&gt;프로그래머스 - 모의고사&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">문제 설명</summary></entry><entry><title type="html">Supervised Learning, Linear Regression</title><link href="https://judepark96.github.io/blog/machine-learning/supervised-learning/2020/05/03/Supervised-Learning-1.html" rel="alternate" type="text/html" title="Supervised Learning, Linear Regression" /><published>2020-05-03T00:00:00-05:00</published><updated>2020-05-03T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/machine-learning/supervised-learning/2020/05/03/Supervised%20Learning-1</id><content type="html" xml:base="https://judepark96.github.io/blog/machine-learning/supervised-learning/2020/05/03/Supervised-Learning-1.html">&lt;h3 id=&quot;1-supervised-learning&quot;&gt;1. Supervised Learning&lt;/h3&gt;

&lt;p&gt;Supervised Learning 이란 주어진 $feature X$ 를 이용하여 $target Y$ 를 예측하는 것이다. $(x_i, y_i)$ 는 하나의 training example 이며 ${(x_i, y_i);i = 1, …, n}$ 를 training set 으로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;위의 Notation 을 바탕으로 정리하자면 주어진 training set 을 바탕으로 $target Y$ 를 예측하는 $h(x)$ 를 학습하는 것이다. 이 $h(x)$ 를 Hypothesis 라고  부른다.&lt;/p&gt;

&lt;p&gt;$target Y$ 가 continuous 하다면 regression problem 이고, $target Y$ 가 discrete 하다면 classification problem 으로 부를 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;2-linear-regression&quot;&gt;2. Linear Regression&lt;/h3&gt;

&lt;p&gt;$X$ 가 2개의 feature 를 가지고 있다고 하자. 그렇다면 $h(x)$ 는 아래와 같이 될 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x) = \theta_0 + \theta_1x_1 + \theta_2x_2&lt;/script&gt;

&lt;p&gt;위의 수식은 linear function 을 통하여 y 로 approximate 하는 것이다. 여기서 $\theta$ 는 parameters 이며 weights 로도 불린다. 이 $\theta$ 는 $x$ 를 $y$ 로 vector space 에서 mapping 하는 것을 의미하며 그렇기 때문에 $\theta$ 가 잘 학습되는 것이 중요하다.&lt;/p&gt;

&lt;p&gt;어떻게 좋은 $\theta$ 를 얻을 수 있을까? Linear Regression 을 하는 목적은 주어진 $feature X$ 를 이용하여 $target Y$ 에 근사시키는 것이다. 이를 위해 &lt;strong&gt;Cost Function&lt;/strong&gt; 을 사용한다. 수식은 아래와 같다.&lt;/p&gt;

&lt;p&gt;$J(\theta) = \frac{1}{2}\sum_{i=1}^n(h_{\theta}(x^{i}) - y^i)^2 $&lt;/p&gt;

&lt;p&gt;Ordinary Least Squares 를 의미하며 수식을 직관적으로 바라보면 $h(x^i)$ 와 $y^i$ 간의 잔차를 제곱한 것이다. 이는 $y$ 와 $h_{\theta}(x)$ 간의 error 를 의미하며 좋은 $\theta$ 를 얻기 위해서 error 를 줄여야할 것이다.&lt;/p&gt;

&lt;p&gt;우리는 $J(\theta)$ 를 최소화(minimize)하는 $\theta$ 를 찾고 싶다. 그렇기 때문에 &lt;strong&gt;Gradient Descent&lt;/strong&gt; 를 사용한다.&lt;/p&gt;

&lt;p&gt;$\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta)$&lt;/p&gt;

&lt;p&gt;$\alpha$ 는 learning rate 를 의미한다. 매 step 마다 $J(\theta)$ 를 최소화할 것이며 결국 수렴할 것이다. right hand side 의 partial derivative term 을 풀어보면 아래와 같다.&lt;/p&gt;

&lt;p&gt;$\frac{\partial}{\partial\theta_j}J(\theta) = (h_\theta(x) - y)x_j$&lt;/p&gt;

&lt;p&gt;그렇기 때문에 update rule 은 아래와 같다.&lt;/p&gt;

&lt;p&gt;$\theta_j := \theta_j + \alpha(y^i-h_{\theta}(x^i)x^i_j))$&lt;/p&gt;

&lt;p&gt;위의 rule 을 &lt;strong&gt;least mean squares update rule&lt;/strong&gt; 이라고 불린다.&lt;/p&gt;

&lt;h3 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Supervised Learning 은 $h(x) :\rightarrow Y$, 즉 $y$ 로 근사시키는 $h(x)$ 를 만드는 것이다.&lt;/li&gt;
  &lt;li&gt;$J(\theta)$ 를 minimize 하기 위하여 Gradient Descent 를 사용한다.&lt;/li&gt;
  &lt;li&gt;Linear Regression 에서 update rule 로서 least mean squares update rule 을 사용한다.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">1. Supervised Learning</summary></entry><entry><title type="html">Real-Time Checking NVIDIA-SMI</title><link href="https://judepark96.github.io/blog/deep-learning/2020/04/30/Real-Time-Checking-NVIDIA-SMI.html" rel="alternate" type="text/html" title="Real-Time Checking NVIDIA-SMI" /><published>2020-04-30T00:00:00-05:00</published><updated>2020-04-30T00:00:00-05:00</updated><id>https://judepark96.github.io/blog/deep-learning/2020/04/30/Real-Time-Checking-NVIDIA-SMI</id><content type="html" xml:base="https://judepark96.github.io/blog/deep-learning/2020/04/30/Real-Time-Checking-NVIDIA-SMI.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;딥러닝 신경망을 학습하다보면 GPU 상태를 확인해볼 필요가 있다. 그 명령어가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvidia-smi&lt;/code&gt; 인데 이거를 매번 타이핑하는 것은 굉장히 비효율적인 행동이다. 리눅스 명령어를 통하여 실시간 모니터링이 가능하도록 하자.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;cuda 관련 환경 구축이 전부 되어있음을 가정한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-monitoring&quot;&gt;2. Monitoring&lt;/h2&gt;

&lt;p&gt;아래의 명령어를 터미널에서 타이핑한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;watch -d -n 0.5 nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러면 아래와 같이 화면이 나온다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/images/nvidia-smi.png&quot; alt=&quot;nvidia-smi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 명령어는 nvidia-smi 를 0.5초에 한 번씩 새로고침되도록 하는 명령어이다. 이를 통하여 실시간 모니터링이 가능하다.&lt;/p&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry></feed>