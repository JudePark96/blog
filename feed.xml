<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://judepark96.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://judepark96.github.io/" rel="alternate" type="text/html" /><updated>2021-01-16T12:33:36-06:00</updated><id>https://judepark96.github.io/feed.xml</id><title type="html">Eunhwan Park X NLP</title><subtitle>Eunhwan Park X NLP</subtitle><entry><title type="html">簡単な卒業の感想</title><link href="https://judepark96.github.io/2020/12/11/%E7%B0%A1%E5%8D%98%E3%81%AA%E5%8D%92%E6%A5%AD%E3%81%AE%E6%84%9F%E6%83%B3.html" rel="alternate" type="text/html" title="簡単な卒業の感想" /><published>2020-12-11T00:00:00-06:00</published><updated>2020-12-11T00:00:00-06:00</updated><id>https://judepark96.github.io/2020/12/11/%E7%B0%A1%E5%8D%98%E3%81%AA%E5%8D%92%E6%A5%AD%E3%81%AE%E6%84%9F%E6%83%B3</id><content type="html" xml:base="https://judepark96.github.io/2020/12/11/%E7%B0%A1%E5%8D%98%E3%81%AA%E5%8D%92%E6%A5%AD%E3%81%AE%E6%84%9F%E6%83%B3.html">&lt;p&gt;そろそろ卒業の時間が来る。&lt;/p&gt;

&lt;p&gt;良いことも、悪い事もありましたけど何か未来が期待される。&lt;/p&gt;

&lt;p&gt;新しいもの研究室に挑戦である。&lt;/p&gt;

&lt;p&gt;未来には良いことだけ！！！&lt;/p&gt;

&lt;p&gt;：ジュドさんは日本語の練習中。
：日本語 Level １。&lt;/p&gt;</content><author><name></name></author><summary type="html">そろそろ卒業の時間が来る。</summary></entry><entry><title type="html">Linear Independence and Dependence</title><link href="https://judepark96.github.io/linear/algebra/2020/11/22/Linear-Independence-and-Dependence.html" rel="alternate" type="text/html" title="Linear Independence and Dependence" /><published>2020-11-22T00:00:00-06:00</published><updated>2020-11-22T00:00:00-06:00</updated><id>https://judepark96.github.io/linear/algebra/2020/11/22/Linear%20Independence%20and%20Dependence</id><content type="html" xml:base="https://judepark96.github.io/linear/algebra/2020/11/22/Linear-Independence-and-Dependence.html">&lt;p&gt;If $S = {v_1, v_2, …, v_r}$ is a nonempty set of vectors in a vector space $V$, then the vector equation:
&lt;script type=&quot;math/tex&quot;&gt;k_1v_1 + k_2v_2+ ... + k_rv_r = 0&lt;/script&gt;
has at least one solution, namely,
&lt;script type=&quot;math/tex&quot;&gt;k_1=0, k_2=0, ..., k_r =0&lt;/script&gt;
We call this the trivial solution. If this is the only solution, then $S$ is said to be a &lt;strong&gt;linearly independent set&lt;/strong&gt;. if there are solutions in addition to the trivial solution then S is said to be a &lt;strong&gt;Linearly dependent set&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Elementary Linear Algebra with Supplemental Applications 11th, Howard Anton, Chris Rorres. 186p&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">If $S = {v_1, v_2, …, v_r}$ is a nonempty set of vectors in a vector space $V$, then the vector equation: has at least one solution, namely, We call this the trivial solution. If this is the only solution, then $S$ is said to be a linearly independent set. if there are solutions in addition to the trivial solution then S is said to be a Linearly dependent set</summary></entry><entry><title type="html">.gitignore 가 동작하지 않아! 어떻게 해야해!?</title><link href="https://judepark96.github.io/github,/git/2020/09/15/gitignore-not-working.html" rel="alternate" type="text/html" title=".gitignore 가 동작하지 않아! 어떻게 해야해!?" /><published>2020-09-15T00:00:00-05:00</published><updated>2020-09-15T00:00:00-05:00</updated><id>https://judepark96.github.io/github,/git/2020/09/15/gitignore-not-working</id><content type="html" xml:base="https://judepark96.github.io/github,/git/2020/09/15/gitignore-not-working.html">&lt;h2 id=&quot;1-어떤-문제가-발생했어&quot;&gt;1. 어떤 문제가 발생했어?&lt;/h2&gt;

&lt;p&gt;나는 항상 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitignore&lt;/code&gt; 를 작성하고 프로젝트를 시작하는데 용량이 큰 바이너리 파일, 불필요한 환경 구축 폴더 등을 올리지 않기 위해서다. 그런데 평소대로 했는데 .gitignore 에 작성한대로 동작을 하지 않았다&lt;/p&gt;

&lt;h2 id=&quot;2-어떻게-해결해&quot;&gt;2. 어떻게 해결해?&lt;/h2&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cached&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
git add &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;remove git cached files&quot;&lt;/span&gt;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이러면 해결이 되더라…!&lt;/p&gt;</content><author><name></name></author><summary type="html">1. 어떤 문제가 발생했어?</summary></entry><entry><title type="html">TorchText Field 의 batch_first 는 항상 True 로 설정하자</title><link href="https://judepark96.github.io/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first.html" rel="alternate" type="text/html" title="TorchText Field 의 batch_first 는 항상 True 로 설정하자" /><published>2020-08-25T00:00:00-05:00</published><updated>2020-08-25T00:00:00-05:00</updated><id>https://judepark96.github.io/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first</id><content type="html" xml:base="https://judepark96.github.io/nlp,/torch,/torchtext/2020/08/25/torchtext-field-batch-first.html">&lt;h2 id=&quot;1-소개&quot;&gt;1. 소개&lt;/h2&gt;

&lt;p&gt;우선 평소에 따로 vocabulary, tokenizer 등의 데이터 전처리 과정에서 필요한 요소들을 직접 코딩해왔었다. 하지만, 이번에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TorchText&lt;/code&gt; 를 한 번 사용해보고자 했다. 그 과정에서 나의 어이없는 실수를 소개한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;한줄 요약: torchtext.data.Field() 를 사용할 때 항상 batch_first=True 로 설정하자.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-어떤-문제가-발생했어&quot;&gt;2. 어떤 문제가 발생했어?&lt;/h2&gt;

&lt;p&gt;우선 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; 학습 데이터를 구축해놓고 이를 TorchText 를 통하여 불렀다. 학습 데이터의 내부 구조는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'text':'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'text':'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label':&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 어떻게 불러오는지 한 번 보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchtext.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TabularDataset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;sos&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eos_token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&amp;lt;eos&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TabularDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data.json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'json'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                                 &lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                                 &lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;우선 SRC와 TRG에 해당하는 Field 를 설정한다. TorchText 에서는 Field 내부의 속성들을 지정함으로서 데이터 전처리 의 번거로움을 줄여준다고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;위의 코드에선 batch_first=True 로 하였다. False 일 때와 차이점을 아래에서 보여줄 예정이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 우리가 일상적으로 사용하는 텐서의 형태로 보기 위해서는 아래와 같이 코드를 작성한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;우리가 만들었어야하는 vocabulary 를 위의 2줄로 만들 수가 있다. 옴마가쉬… 그 외에도 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_freq&lt;/code&gt; 와 같은 속성으로 vocabulary 생성 과정에 조건을 걸 수 있다.&lt;/p&gt;

&lt;p&gt;이제 iterator 로 돌려보고 결과를 보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_iterator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BucketIterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BATCH_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TRG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text -&amp;gt; to &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; , a large number of ... have been stated and the most appropriate algorithm &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;each application has been determined &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

label -&amp;gt; &amp;lt;sos&amp;gt; autonomous exploration __&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;__ mapping of unknown environments &amp;lt;eos&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;대충 의도한대로 제대로 나왔다. 그렇다면 만약 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_first=False&lt;/code&gt; 로 설정한다면 어떻게 될까?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;텍스트 길이가 길기 때문에 일부 텍스트와 패딩 토큰을 보기 좋게 하기 위하여 지웠다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt;
&amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt; &amp;lt;sos&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_first=False&lt;/code&gt; 를 하였을 때는 위처럼 나오게 된다. 옴마가쉬…&lt;/p&gt;

&lt;h2 id=&quot;3-여기서-나의-실수는&quot;&gt;3. 여기서 나의 실수는?&lt;/h2&gt;

&lt;p&gt;많은 예제들, 그리고 나의 경험에서 나는 항상 데이터 전처리 과정에서 텐서의 형상을 $\text{batch_size} \ntimes \text{sequence_length}$ 의 형태를 띄도록 하였다. 하지만 TorchText 에서는 $\text{sequence_length} \ntimes \text{batch_size}$ 의 형태 또한 지원했던 것이다. 그렇기 때문에 Index -&amp;gt; Text 로 변환하였을 때 제대로 표현이 안되었던 것이다. 홀리쉿…&lt;/p&gt;

&lt;h2 id=&quot;4-결론&quot;&gt;4. 결론&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;batch_first&lt;/strong&gt; 속성을 확인하자.&lt;/li&gt;
  &lt;li&gt;TorchText 는 좋은 거다. 나름대로.&lt;/li&gt;
  &lt;li&gt;토큰화 과정은 &lt;strong&gt;따로 전처리하는 것&lt;/strong&gt;이 나은 것 같다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. 소개</summary></entry><entry><title type="html">Adding Special Tokens To HuggingFace BertTokenizer</title><link href="https://judepark96.github.io/nlp/2020/08/18/Adding-Special-Tokens-To-HuggingFace-BertTokenizer.html" rel="alternate" type="text/html" title="Adding Special Tokens To HuggingFace BertTokenizer" /><published>2020-08-18T00:00:00-05:00</published><updated>2020-08-18T00:00:00-05:00</updated><id>https://judepark96.github.io/nlp/2020/08/18/Adding%20Special%20Tokens%20To%20HuggingFace%20BertTokenizer</id><content type="html" xml:base="https://judepark96.github.io/nlp/2020/08/18/Adding-Special-Tokens-To-HuggingFace-BertTokenizer.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Sometimes, except pre-defined special tokens, such as &lt;strong&gt;[SEP], [CLS]&lt;/strong&gt;, we need to add special tokens to BertTokenizer. &lt;strong&gt;In this post, I Introduce the way to add special tokens to BertTokenizer.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-method&quot;&gt;2. Method&lt;/h2&gt;

&lt;p&gt;First, we need to define speical token what we will add. In this post, adding special token will be &lt;strong&gt;‘[eou]’&lt;/strong&gt; which means &lt;strong&gt;End of Utterances&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eou_token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'[eou]'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, add eou_token to BertTokenizer.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;special_tokens_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'additional_special_tokens'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode_sep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;orig_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_added_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_special_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;special_tokens_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orig_num_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_added_tokens&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We got the tokenizer we added the special tokens, and total number of vocab. After this stage, we need to change the embedding size of BertModel.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize_token_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_num_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Done!&lt;/p&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Graph Convolutional Networks for Text Classification</title><link href="https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph-Convolutional-Networks-for-Text-Classification.html" rel="alternate" type="text/html" title="Graph Convolutional Networks for Text Classification" /><published>2020-08-14T00:00:00-05:00</published><updated>2020-08-14T00:00:00-05:00</updated><id>https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph%20Convolutional%20Networks%20for%20Text%20Classification</id><content type="html" xml:base="https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp,/graph-neural-network/2020/08/14/Graph-Convolutional-Networks-for-Text-Classification.html">&lt;h1 id=&quot;1-초록은-뭐라고-말하고-있어-&quot;&gt;1. 초록은 뭐라고 말하고 있어 ?&lt;/h1&gt;

&lt;p&gt;기존의 텍스트 분류 과업을 하기 위한 몇 몇 연구에서는 convolutional neural networks 를 사용하였다. 본 논문에서는 convolutional neural networks 가 아닌 graph convolutional networks 를 텍스트 분류 과업에 적용하는 것을 제안한다.&lt;/p&gt;

&lt;h1 id=&quot;2-주요-기여점은-뭐야-&quot;&gt;2. 주요 기여점은 뭐야 ?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;We propose a novel graph neural network method for text classification. To the best of our knowledge, this is the first study to model a whole corpus as a heterogeneous graph and learn word and document embeddings with graph neural networks jointly.&lt;/li&gt;
  &lt;li&gt;Results on several benchmark datasets demonstrate that our method outperforms state-of-the-art text classification methods, without using pre-trained word embeddings or external knowledge. Our method also learn predictive word and document embeddings automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3-이전의-접근과는-뭐가-다른-것-같아-&quot;&gt;3. 이전의 접근과는 뭐가 다른 것 같아 ?&lt;/h1&gt;

&lt;p&gt;기존의 텍스트 분류 과업을 위한 주요 딥러닝 모델로는 CNN, LSTM 등이 있다. 이러한 모델들은 local consecutive word sequences 의 semantic, synthetic information 을 잘 caputre 할 수 있지만 global word co-occurence 를 무시할 수 있는 가능성이 있다.&lt;/p&gt;

&lt;p&gt;본 논문에서는 이전의 접근과는 다르게 graph representation 을 학습하여 classification task 를 수행한다는 점이다. 이를 수행하기 위하여 corpus -&amp;gt; graph 의 방법으로 PMI, TF-IDF 를 소개하였고, PMI 의 경우가 성능이 더 좋음을 알려준다. 또한 two-layer GCN 이 one-layer GCN 보다 성능이 좋았지만 더 많은 layer 는 성능에 큰 의미가 없음을 나타내었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;기존의 CNN, LSTM 을 GNN 계열의 신경망으로 대체한다는 것 자체가 geometric 의 관점으로 보았을 때 큰 차이가 있다.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-어떤-걸-제안할-수-있을까-&quot;&gt;4. 어떤 걸 제안할 수 있을까 ?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;encoding method (corpus -&amp;gt; graph)&lt;/li&gt;
  &lt;li&gt;residual connection&lt;/li&gt;
  &lt;li&gt;한국어 데이터 집합에 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;5-reference&quot;&gt;5. Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;&gt;Graph Convolutional Networks for Text Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. 초록은 뭐라고 말하고 있어 ?</summary></entry><entry><title type="html">What is Page Rank?</title><link href="https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html" rel="alternate" type="text/html" title="What is Page Rank?" /><published>2020-08-12T00:00:00-05:00</published><updated>2020-08-12T00:00:00-05:00</updated><id>https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR</id><content type="html" xml:base="https://judepark96.github.io/graph-representation,/graph,/page-rank,/nlp/2020/08/12/PR.html">&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;PageRank (PR) is an algorithm used to calculate the weight for web pages.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;2-pr-equation&quot;&gt;2. PR Equation&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(V_i) = (1-d) + d * \sum_{j\in In(v_i)}\frac{1}{|Out(v_j)|}S(V_j)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Denotes
    &lt;ul&gt;
      &lt;li&gt;$S(V_i)$ ← the weight of webpage $i$&lt;/li&gt;
      &lt;li&gt;$d$ ← damping factor, in case of no outgoing links&lt;/li&gt;
      &lt;li&gt;$In(v_i)$ ← inbound links of $i$, which is a set.&lt;/li&gt;
      &lt;li&gt;$Out(v_j)$ ← outgoing links of $j$, which is a set.&lt;/li&gt;
      &lt;li&gt;$\vert Out(v_j) \vert$ ← the number of outbound links&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;21-example&quot;&gt;2.1. Example&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1030/1*DkZjlRNEjPSc8RNL7yWggA.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Weight of node $e$ is below as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;In(v_e) = [a, b], j\in [a, b] \\
S(V_e) = (1-d) + d*(S(V_a) + \frac{1}{2}S(V_b))&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;In the initialization, the weight of each node is $1$.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-pagerank-for-keyword-extraction-by-python&quot;&gt;3. PageRank for Keyword Extraction by Python&lt;/h1&gt;

&lt;p&gt;You can see the code at &lt;a href=&quot;https://github.com/JudePark96/textrank-nlp&quot;&gt;textrank-nlp&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0&quot;&gt;Understand TextRank for Keyword Extraction by Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Machine Translation, Sequence-to-Sequence and Attention</title><link href="https://judepark96.github.io/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html" rel="alternate" type="text/html" title="Machine Translation, Sequence-to-Sequence and Attention" /><published>2020-08-01T00:00:00-05:00</published><updated>2020-08-01T00:00:00-05:00</updated><id>https://judepark96.github.io/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine%20Translation,%20Sequence-to-Sequence%20and%20Attention</id><content type="html" xml:base="https://judepark96.github.io/nlp,/nmt,/seq2seq,/attention/2020/08/01/Machine-Translation,-Sequence-to-Sequence-and-Attention.html">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Seq2Seq 공부를 하는데 블로그들은 알아보기 &lt;strong&gt;(내 기준)&lt;/strong&gt; 힘들어서 CS224n 의 강의 슬라이드를 보면서 정리하였다. 근데 내 정리도 보니까 나 혼자밖에 못 알아볼 듯.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/JudePark96/blog/master/_posts/nmt/Page6.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-conclusion&quot;&gt;2. Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Neural Machine Translation 은 Sequence-to-Sequence 형태의 Neural Network 로 접근함.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Dialogue, Summarization 등의 다양한 Task 에 사용될 수 있음.&lt;/li&gt;
  &lt;li&gt;Sequence-to-Sequence 는 Conditional Language Model 이라고 볼 수 있음.&lt;/li&gt;
  &lt;li&gt;Decoding 에서 항상 optimal solution 이라고 보장할 수는 없지만 beam search 를 이용하는 것이 효율적임.&lt;/li&gt;
  &lt;li&gt;Evaluation Metric 으로서 BLEU 를 사용하지만 불완전함.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://judepark96.github.io/blog/2020/03/22/BLEU-Score-Reasonable.html&quot;&gt;BLEU (Bilingual Evaluation Understudy) Score&lt;/a&gt; 를 참고.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Information Bottleneck 을 Attention Mecahnism 을 이용하여 해소함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture08-nmt.pdf&quot;&gt;CS224n Lecture 8 Slide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Introduction</summary></entry><entry><title type="html">Ambiguity In Sentence Structure</title><link href="https://judepark96.github.io/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html" rel="alternate" type="text/html" title="Ambiguity In Sentence Structure" /><published>2020-07-28T00:00:00-05:00</published><updated>2020-07-28T00:00:00-05:00</updated><id>https://judepark96.github.io/nlp/2020/07/28/Ambiguity%20In%20Sentence%20Structure</id><content type="html" xml:base="https://judepark96.github.io/nlp/2020/07/28/Ambiguity-In-Sentence-Structure.html">&lt;h3 id=&quot;1-attachment-ambiguity&quot;&gt;1. Attachment ambiguity&lt;/h3&gt;

&lt;p&gt;A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.&lt;/p&gt;

&lt;h3 id=&quot;2-coordination-ambiguity&quot;&gt;2. Coordination ambiguity&lt;/h3&gt;

&lt;p&gt;In coordination ambiguity different sets of phrases can be conjoined by a con- junction like &lt;em&gt;and&lt;/em&gt;. For example, the phrase &lt;strong&gt;&lt;em&gt;old men and women&lt;/em&gt;&lt;/strong&gt; can be bracketed as &lt;em&gt;[old [men and women]]&lt;/em&gt;, referring to &lt;strong&gt;&lt;em&gt;old men&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;old women&lt;/em&gt;&lt;/strong&gt;, or as &lt;strong&gt;&lt;em&gt;[old men] and [women]&lt;/em&gt;,&lt;/strong&gt; in which case it is only the men who are old.&lt;/p&gt;

&lt;h3 id=&quot;3-reference&quot;&gt;3. Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">1. Attachment ambiguity</summary></entry><entry><title type="html">하반기 연구 주제를 위한 공부 목록</title><link href="https://judepark96.github.io/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html" rel="alternate" type="text/html" title="하반기 연구 주제를 위한 공부 목록" /><published>2020-07-06T00:00:00-05:00</published><updated>2020-07-06T00:00:00-05:00</updated><id>https://judepark96.github.io/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D</id><content type="html" xml:base="https://judepark96.github.io/%EC%97%B0%EA%B5%AC/2020/07/06/%ED%95%98%EB%B0%98%EA%B8%B0-%EC%97%B0%EA%B5%AC-%EC%A3%BC%EC%A0%9C%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B3%B5%EB%B6%80-%EB%AA%A9%EB%A1%9D.html">&lt;ul&gt;
  &lt;li&gt;연구 주제
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Keyphrase 추출/생성&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Named Entity Recognition&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;공부 목록
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Stanford NLP&lt;/a&gt; chap 2-3, 8, 12-15&lt;/li&gt;
      &lt;li&gt;관련 논문 읽기&lt;/li&gt;
      &lt;li&gt;ICLR 2020 최근 논문 읽기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해야할 일
    &lt;ul&gt;
      &lt;li&gt;연구 주제 실험&lt;/li&gt;
      &lt;li&gt;다양한 연구 주제 생각&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">연구 주제 Keyphrase 추출/생성 Named Entity Recognition 공부 목록 Stanford NLP chap 2-3, 8, 12-15 관련 논문 읽기 ICLR 2020 최근 논문 읽기 해야할 일 연구 주제 실험 다양한 연구 주제 생각</summary></entry></feed>